# -*- coding: utf-8 -*-
"""datathon_ml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eykvIrJWBd0BDl97eNzdcSTZVGWRFES2
"""

from google.colab import files
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
import numpy as np

upload = files.upload()

with open('test (3).csv', 'r') as file:
  test_df = file.read().splitlines()

# Read the CSV data into a DataFrame
test_df = pd.read_csv(io.StringIO(upload['test (3).csv'].decode('utf-8')))

test_df

upload = files.upload()

with open('training (1).csv', 'r') as file:
  train_df = file.read().splitlines()

# Read the CSV data into a DataFrame
train_df = pd.read_csv(io.StringIO(upload['training (1).csv'].decode('utf-8')))

train_df

train_df_null = train_df.isnull().sum()
train_df_null_counts = train_df_null.sort_values(ascending=False)
train_df_null_counts

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
train_df_null_counts

train_df.describe()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.histplot(train_df['patient_age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8,4))
sns.histplot(train_df['bmi'].dropna(), bins=30, kde=True)
plt.title('BMI Distribution')
plt.xlabel('BMI')
plt.ylabel('Frequency')
plt.show()

# Correlation matrix heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(train_df.corr(), annot=False, cmap='coolwarm')
plt.title('Correlation Matrix')

plt.show()

race_counts = train_df['patient_race'].value_counts()
race_names = race_counts.index
race_values = race_counts.values

race_counts

sns.set_palette("Set2")

plt.figure(figsize=(8,8))
plt.pie(race_values, labels=race_names, autopct='%1.1f%%', startangle=140)
plt.title('patient Race Distribution')
plt.axis('equal')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

categorical_cols = train_df.select_dtypes(include=['object']).columns

train_df_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)
test_df_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)

train_df_encoded, test_df_encoded = train_df_encoded.align(test_df_encoded, join='inner', axis=1)

train_df_encoded['DiagPeriodL90D'] = train_df['DiagPeriodL90D']

train_df_encoded.fillna(train_df_encoded.median(), inplace=True)
test_df_encoded.fillna(test_df_encoded.median(), inplace=True)

X = train_df_encoded.drop('DiagPeriodL90D', axis=1)
y = train_df_encoded['DiagPeriodL90D']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_val_scaled)

accuracy = accuracy_score(y_val, y_pred)
roc_auc = roc_auc_score(y_val, y_pred)
print(f'Accuracy: {accuracy}, ROC-AUC: {roc_auc}')

palette = sns.color_palette("husl", 20)
feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)
feature_importances.nlargest(20).plot(kind='barh', color=palette)
plt.title('Top 20 Important Features')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Your existing code to create the feature_importances Series
feature_importances = pd.Series(model.feature_importances_, index=X_train.columns)

# Plot the top 10 important features in a pie chart
plt.figure(figsize=(8, 8))
plt.pie(feature_importances.nlargest(10), labels=feature_importances.nlargest(10).index, autopct='%1.1f%%', startangle=100, colors=sns.color_palette("husl", 10))
plt.title('Top 10 Important Features')
plt.axis('equal')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Create age bins in 10-year intervals
age_bins = range(0, 100, 10)

# Bin the ages into the specified intervals
binned_ages = pd.cut(train_df['patient_age'], bins=age_bins)

# Count the number of patients in each age interval
age_counts = binned_ages.value_counts().sort_index()

# Plot the pie chart
plt.figure(figsize=(8, 8))
plt.pie(age_counts, labels=age_counts.index, autopct='%1.1f%%', startangle=190)
plt.title('Patient Age Distribution (10-Year Intervals)')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10,6))
sns.countplot(x = 'patient_race', hue = 'DiagPeriodL90D', data = train_df)
plt.title('Diagnosis Period Comparison by Race')
plt.xlabel('Patient Race')
plt.ylabel('Count')
plt.show()

environmental_columns = ['Ozone', 'PM25', 'N02', 'DiagPeriodL90D']
correlation = train_df[environmental_columns].corr()

# Correlation matrix heatmap
#plt.figure(figsize=(12, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')

plt.show()

counts = train_df['breast_cancer_diagnosis_desc'].value_counts()
plt.figure(figsize=(10,6))
sns.barplot(x = counts.index, y=counts.values)
plt.title("Distribution of Cancer Diagnoses")
plt.xlabel('Diagnosis')
plt.ylabel('Frequency')
plt.xticks(rotation=45, fontsize=6, ha='right')
plt.yticks(fontsize = 10)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Identify numeric and categorical columns
numeric_features = train_df.select_dtypes(include=['int64', 'float64']).columns.drop('DiagPeriodL90D')
categorical_features = train_df.select_dtypes(include=['object']).columns

# Preprocessing for numeric data: impute missing values with mean
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

# Preprocessing for categorical data: impute missing values with most frequent and apply one-hot encoding
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Bundle preprocessing for numeric and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Define the model
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', LogisticRegression(max_iter=1000))])

# Separate target variable and features in training data
X_train = train_df.drop('DiagPeriodL90D', axis=1)
y_train = train_df['DiagPeriodL90D']

# Split the training data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=0)

# Train the model
model.fit(X_train_split, y_train_split)

# Predict on validation set
y_pred = model.predict(X_val_split)

# Calculate accuracy
accuracy = accuracy_score(y_val_split, y_pred)

accuracy

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'home_ownership' is treated as a numeric column
train_df['home_ownership'] = pd.to_numeric(train_df['home_ownership'], errors='coerce')

# Drop rows where either 'home_ownership' or 'DiagPeriodL90D' is NaN to ensure clean comparison
cleaned_df = train_df.dropna(subset=['home_ownership', 'DiagPeriodL90D'])

# Create a box plot
plt.figure(figsize=(10, 6))
sns.boxplot(x='DiagPeriodL90D', y='home_ownership', data=cleaned_df)
plt.title('Comparison of Home Ownership Between Patients Diagnosed Within and After 90 Days')
plt.xlabel('Diagnosed Within 90 Days (0 = No, 1 = Yes)')
plt.ylabel('Percentage of Home Ownership')
plt.show()

